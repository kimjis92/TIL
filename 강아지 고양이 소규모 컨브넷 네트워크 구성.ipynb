{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil   \n",
    "#os 모듈은 운영 체제와 상호 작용하기위한 수십 가지 함수들을 제공\n",
    "#shutil 파일에 관한 기능을 가지고 있는 함수들을 제공\n",
    "# https://docs.python.org/ko/3/library/shutil.html 여러 파이썬라이브러리함수들을 살펴볼수 있음\n",
    "\n",
    "original_dataset_dir='./datasets/cats_and_dogs/train/train'\n",
    "\n",
    "base_dir='./datasets/cats_and_dogs_small'\n",
    "#os.mkdir(base_dir) #  os.mkdir() -->폴더생성\n",
    "\n",
    "train_dir=os.path.join(base_dir, 'train') # os.path.join()--> 경로를 합쳐서 새 경로 생성\n",
    "#os.mkdir(train_dir)\n",
    "validation_dir=os.path.join(base_dir, 'validation')\n",
    "#os.mkdir(validation_dir)\n",
    "test_dir=os.path.join(base_dir, 'test')\n",
    "#os.mkdir(test_dir)\n",
    "\n",
    "train_cats_dir=os.path.join(train_dir, 'cats')\n",
    "#os.mkdir(train_cats_dir)\n",
    "\n",
    "train_dogs_dir=os.path.join(train_dir, 'dogs')\n",
    "#os.mkdir(train_dogs_dir)\n",
    "\n",
    "validation_cats_dir=os.path.join(validation_dir, 'cats')\n",
    "#os.mkdir(validation_cats_dir)\n",
    "\n",
    "validation_dogs_dir=os.path.join(validation_dir, 'dogs')\n",
    "#os.mkdir(validation_dogs_dir)\n",
    "\n",
    "test_cats_dir=os.path.join(test_dir, 'cats')\n",
    "#os.mkdir(test_cats_dir)\n",
    "\n",
    "test_dogts_dir=os.path.join(test_dir, 'dogs')\n",
    "#os.mkdir(test_dogts_dir)\n",
    "\n",
    "# fnames =['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "# for fname in fnames:\n",
    "#     src=os.path.join(original_dataset_dir, fname)\n",
    "#     dst=os.path.join(train_cats_dir, fname)\n",
    "#     shutil.copyfile(src, dst) # shutll.copyfile() 는 src파일의 내용을 dst파일에게 복사한다.\n",
    "    \n",
    "# fnames =['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "# for fname in fnames:\n",
    "#     src=os.path.join(original_dataset_dir, fname)\n",
    "#     dst=os.path.join(validation_cats_dir, fname)\n",
    "#     shutil.copyfile(src, dst)\n",
    "    \n",
    "# fnames =['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "# for fname in fnames:\n",
    "#     src=os.path.join(original_dataset_dir, fname)\n",
    "#     dst=os.path.join(test_cats_dir, fname)\n",
    "#     shutil.copyfile(src, dst)\n",
    "\n",
    "# #아래는 dog\n",
    "\n",
    "# fnames =['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "# for fname in fnames:\n",
    "#     src=os.path.join(original_dataset_dir, fname)\n",
    "#     dst=os.path.join(train_dogs_dir, fname)\n",
    "#     shutil.copyfile(src, dst) # shutll.copyfile() 는 src파일의 내용을 dst파일에게 복사한다.\n",
    "    \n",
    "# fnames =['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "# for fname in fnames:\n",
    "#     src=os.path.join(original_dataset_dir, fname)\n",
    "#     dst=os.path.join(validation_dogs_dir, fname)\n",
    "#     shutil.copyfile(src, dst)\n",
    "    \n",
    "# fnames =['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "# for fname in fnames:\n",
    "#     src=os.path.join(original_dataset_dir, fname)\n",
    "#     dst=os.path.join(test_dogts_dir, fname)\n",
    "#     shutil.copyfile(src, dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 고양이 이미지 전체 개수: 1000\n",
      "훈련용 강아지 이미지 전체 개수: 1000\n",
      "검증용 고양이 이미지 전체 개수: 500\n",
      "검증용 강아지 이미지 전체 개수: 500\n",
      "테스트용 고양이 이미지 전체 개수: 500\n",
      "테스트용 강아지 이미지 전체 개수: 500\n"
     ]
    }
   ],
   "source": [
    "#os.listdir(): 디렉토리 안의 파일, 서브디렉토리 리스트\n",
    "print('훈련용 고양이 이미지 전체 개수:', len(os.listdir(train_cats_dir)))\n",
    "print('훈련용 강아지 이미지 전체 개수:', len(os.listdir(train_dogs_dir)))\n",
    "\n",
    "print('검증용 고양이 이미지 전체 개수:', len(os.listdir(validation_cats_dir)))\n",
    "print('검증용 강아지 이미지 전체 개수:', len(os.listdir(validation_dogs_dir)))\n",
    "\n",
    "print('테스트용 고양이 이미지 전체 개수:', len(os.listdir(test_cats_dir))) \n",
    "print('테스트용 강아지 이미지 전체 개수:', len(os.listdir(test_dogts_dir))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                       input_shape=(150, 150,3)))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation='relu'))   #layers.Dense() : densely-connected NN layer\n",
    "model.add(layers.Dense(1, activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마지막 레이어에 활성화 함수에 sigmoid 함수를 쓴 이유<br />\n",
    "중간 히든레이어의 활성화 함수에 sigmoid 함수를 쓰면 학습속도 저하 Gradient Vanishing등의 문제가 생긴다. 하지만 마지막레이어에 sigmoid함수를 활성화 함수에 쓰는 이유는 sigmoid함수는 binary classification 에 적절함 함수다. 일정 값을 기준으로 0인지 1인지구분함으로써 분류하는 방식이다. 지금 위의 모델은 강아지와 고양이를 분류하는 binary classification 이므로 마지막 레이어에 sigmoid함수를 활성화 함수에 쓴것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actvation 함수들의 이해 \n",
    "### https://reniew.github.io/12/ 사이트를 참조 했습니다.<br />  \n",
    "선형함수인 h(x)=cx를 활성화함수로 사용한 3층 네트워크를 떠올려 보면  y(x)=h(h(h(x)))가 되는데 이는 실은 y(x)=ax와 똑같은 식, a=c3이라고만 하면 끝이다. 즉, 은닉층이 없는 네트워크로 표현할 수 있다. 뉴럴네트워크에서 층을 쌓는 혜택을 얻고 싶다면 활성화함수로는 반드시 비선형 함수를 사용해야 한다.\n",
    "<br />   \n",
    "비선형 함수에는 대표적으로 3가지 함수가 있다.<br />   \n",
    "1.시그모이드 함수(Sigmoid)<br />\n",
    "![Sigmoid 공식](./screenshot/sigfun.PNG)\n",
    "![Sigmoid 함수](./screenshot/sig.PNG)\n",
    "![Sigmoid 미분함수](./screenshot/sigder.PNG)\n",
    "<br/>\n",
    "sigmoid 함수는 초창기에는 많이 쓰였다. 하지만 단점도 있다.<br /> \n",
    "\n",
    "---미분함수의 최대값은 4분의 1로 몇번 레이어를 거치다보면 값은 0에 수렴할 수 있고 절대값이 크면 클수록 역시 0에 수렴한다. 이를 Gradient Vanishing 현상이 발생한다고 한다.\n",
    "\n",
    "<br/>\n",
    "---학습속도가 느리다\n",
    "\n",
    "<br />  \n",
    "2.tanh함수<br />  \n",
    "![Tanh 함수](./screenshot/tanhfun.PNG)\n",
    "![Tanh 미분함수](./screenshot/tanhder.PNG)\n",
    "<br />  \n",
    "위의 시그모이드함수의 2번째 단점을 개선했으나 미분함수를 보면 알다시피 Gradient Vanishing 현상을 개선하지는 못했다.\n",
    "<br />  \n",
    "3.relu함수\n",
    "![Relu 함수](./screenshot/relu.PNG)\n",
    "<br />  \n",
    "relu함수는 위 두함수에 비해 학습속도 개선및 Gradient Vanishing 현상을 어느정도 해결했으나 x<0 인값들에 대해서는 기울기가 0이기 때문에 뉴런이 죽을 수 있는 단점이 존재한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizerszers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4), #lr: learning late\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimzer에 대한 빠른개요\n",
    "![optimizer 대략적 계보](./screenshot/optimizer.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#train_datagen=ImageDataGenerator(rescale=1./255) # 데이터아규멘테이션 안함\n",
    "\n",
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range==0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "test_datagen=ImageDataGenerator(reascale=1./255) \n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator=test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "history=model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=100,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)\n",
    "\n",
    "#위는 데이터 아규멘테이션 적용한것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataGenerator <br >\n",
    "\n",
    " 위 클래스는 이미지 파일을 전처리된 배치 텐서로 자동으로 바꾸어주는 파이썬 제너레이터를 만들어 준다.\n",
    "일반적으로 전처리를 할때 위 클래스를 사용하는 가장 큰이유는 데이터 아규멘테이션을 하기 위해서다. 그와 관련한 자세한 내용은 https://keras.io/preprocessing/image/ 에서 확인할 수 있습니다. 간단하게 몇가지 변수만 보자면 featurewise, samplewise 같은경우는 표준화 방법이다. 쉽게 생각하자면 위 클래스는 믹서기의 종류이고 클래스의 객체 flow_from_directory같은 메소드 들로 내용물(데이터)을 형성하는것이다.\n",
    "\n",
    "<br >\n",
    "\n",
    "### model.fit_generator <br >\n",
    "\n",
    "model.fit과 마찬가지로 model.fit_generator는 제너레이터들을 학습시켜준다. history 객체를 리턴한다. 히스토리 객체는 각 epoch에 대한 loss, acc 값을 train, validation 에 대해 각각 값을 갖고 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cats_and_dogs_small_3.h5\")# 케라스에서는 모델 아키텍쳐와 모델 가중치를 h5파일 형식으로 저장\n",
    "#관련해서는 사이트 https://tykimos.github.io/2017/06/10/Model_Save_Load/ 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 훈련된 컨브넷 사용하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 기반층, 완전연결 분류기 이렇게 모델을 2가지 부분으로 나눈다면 합성곱 기반층만 재사용하는걸 권장한다.<br >\n",
    "\n",
    "<br >\n",
    "\n",
    " 왜냐면 컨브넷의 특성맵은 사진에 대한 일반적인 개념의 존재 여부를 기록한 맵으로 합성곱 층에 의해 학습된 표현이 일반적이어서 재사용이 가능하기 때문이다.<br >\n",
    " \n",
    " <br >\n",
    " \n",
    " 분류기는 전체 사진에 어떤 클래스가 존재할 확률에 관한 정보만 담고 있다. <br >\n",
    " \n",
    "<br >\n",
    " \n",
    " 완전 연결 층들은 공간 개념을 제거하여, 완전 연결층에서 찾은 표현은 더 이상 입력 이미지에 있는 객체의 위치정보를 가지고 있지 않는다. 합성곱의 특성맵은 객체 위치를 고려한다. <br >\n",
    " \n",
    " <br >\n",
    " \n",
    " 모델의 하위층은 (에지, 색깔, 질감 등) 지역적이고 매우 일반적인 특성맵을 추출하고, 상위층은('강아지 눈'이나 '고양이 귀'처럼)좀 더 추상적인 개념을 추출한다. 따라서 새로운 데이터셋이 원본 모델이 훈련한 데이터셋과 많이 다르다면 모델의 하위 층 몇 개만 특성 추출에 사용하는것이 좋다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base=VGG16(weights='imagenet', #체크포인트  ex) imagenet에서 미리 학습시킨 가중치 사용\n",
    "               include_top=False, #최상위 완전연결 분류기 포함할지 안할지 결정\n",
    "               input_shape=(150, 150, 3)) # 아무것도 지정하지 않으면 어떤 크기의 입력도 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 16,812,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 컴파일하고 훈련하기전에 합성곱 기반 층을 동결하는것이 중요하다. 만약 동결하지 않으면 훈련하는동안 가중치가 업데이트 되는데 처음에 dense층의 랜덤하게 생성된 큰가중치 값들이 영향을줘 사전에 학습된 네트워크를 망친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_base를 동결하기 전 훈련되는 가중치의 수: 30\n",
      "conv_base를 동결한 후 훈련되는 가중치의 수: 4\n"
     ]
    }
   ],
   "source": [
    "print('conv_base를 동결하기 전 훈련되는 가중치의 수:', len(model.trainable_weights))\n",
    "conv_base.trainable=False  # 합성곱 기반층 동결\n",
    "print('conv_base를 동결한 후 훈련되는 가중치의 수:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      " - 324s - loss: 0.5039 - acc: 0.7805 - val_loss: 0.3933 - val_acc: 0.8550\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-18aeeb1f8808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     verbose=2)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#위는 데이터 아규멘테이션 적용한것\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    235\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    344\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "#train_datagen=ImageDataGenerator(rescale=1./255) # 데이터아규멘테이션 안함\n",
    "\n",
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator=test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=2e-5), #lr: learning late\n",
    "             metrics=['acc'])\n",
    "\n",
    "history=model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50,\n",
    "    verbose=2)\n",
    "\n",
    "#위는 데이터 아규멘테이션 적용한것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
